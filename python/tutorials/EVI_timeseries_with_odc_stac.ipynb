{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an EVI Timeseries with ODC STAC\n",
    "\n",
    "This tutorial provides an alternate way to work with Harmonized Landsat Sentinel-2 ([HLS](https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/)) data using [CMR-STAC](https://cmr.earthdata.nasa.gov/stac/docs/index.html) coupled and [OpenDataCube](https://www.opendatacube.org/), which uses the [dask](https://www.dask.org/) and [xarray](https://xarray.dev/) libraries to build a timeseries of data. `OpenDataCube` leverages the STAC metadata to provide values to `xarray` so that data can be loaded lazily with all of the dimensions, coordinate reference system and other relevant attributes. This information along with the cloud-optimized geotiff format of HLS Data means that only the necessary portions of the files required for the desired operation need to be read. On top of this, `dask` is used for parallelization. This workflow is a much faster alternative to the methods used in the [HLS Tutorial](https://github.com/nasa/HLS-Data-Resources/blob/main/python/tutorials/HLS_Tutorial.ipynb), but uses higher level Python libraries, making the code less adaptable to other use-cases. Using these additional Python libraries requires additional dependencies from other resources in this repository. Please use the [Python Environment Setup](#python-environment-setup) section below to set up a compatible Python environment.\n",
    "\n",
    "## Python Environment Setup\n",
    "\n",
    "A compatible python environment can be created by following the [Python Environment setup instructions](https://github.com/nasa/VITALS/blob/main/setup/setup_instructions.md), activating that environment and adding the `pystac-client` and `odc-stac` packages:\n",
    "\n",
    "```\n",
    "mamba activate lpdaac_vitals\n",
    "```\n",
    "\n",
    "```\n",
    "mamba install -c conda-forge pystac-client odc-stac dask\n",
    "```\n",
    "\n",
    "or if you want to set up a separate environment:\n",
    "\n",
    "```\n",
    "mamba create -n hls_odc -c conda-forge --yes python=3.12 fiona gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh scikit-image jupyterlab dask odc-stac pystac-client\n",
    "```\n",
    "\n",
    "## Tutorial Use Case\n",
    "\n",
    "This guide examines changes in enhanced vegetation index ([EVI](https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_4.php)) over an agricultural region in northern California, the same as the [HLS Tutorial: Getting Started with Cloud-Native HLS Data in Python](https://github.com/nasa/HLS-Data-Resources/blob/main/python/tutorials/HLS_Tutorial.ipynb). The goal of the project is to observe HLS-derived mean EVI over these regions without downloading the entirety of the HLS source data in a cloud-friendly, efficient way. In this notebook we will extract an EVI timeseries from Harmonized Landsat Sentinel-2 ([HLS](https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/)) data. First we will search and find data, then we will lazily load only the necessary subsets of these results directly from the cloud and calculate EVI. \n",
    "\n",
    "### Background\n",
    "\n",
    "The Harmonized Landsat Sentinel-2 ([HLS](https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/)) project produces seamless, harmonized surface reflectance data from the Operational Land Imager (OLI) and Multi-Spectral Instrument (MSI) aboard Landsat and Sentinel-2 Earth-observing satellites, respectively. The aim is to produce seamless products with normalized parameters, which include atmospheric correction, cloud and cloud-shadow masking, geographic co-registration and common gridding, normalized bidirectional reflectance distribution function, and spectral band adjustment. This will provide global observation of the Earth’s surface every 2-3 days with 30 meter spatial resolution. One of the major applications that will benefit from HLS is agriculture assessment and monitoring, which is used as the use case for this tutorial.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- A [NASA Earthdata Login](https://urs.earthdata.nasa.gov/) account is required to download the data used in this tutorial. You can create an account at the link provided.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- How to use CMR-STAC to search for HLS data\n",
    "- How to open and access data using dask and Open Data Cube.\n",
    "\n",
    "## Data Used\n",
    "\n",
    "- Daily 30 meter (m) global HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance - [HLSS30.002](https://doi.org/10.5067/HLS/HLSS30.002)  \n",
    "    - The HLSS30 product provides 30 m Nadir normalized Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Sentinel-2A and Sentinel-2B MSI data products.  \n",
    "    - Science Dataset (SDS) layers:  \n",
    "        - B8A (NIR Narrow)  \n",
    "        - B04 (Red)  \n",
    "        - B02 (Blue)   \n",
    "- Daily 30 meter (m) global HLS Landsat-8 OLI Surface Reflectance - [HLSL30.002](https://doi.org/10.5067/HLS/HLSL30.002)  \n",
    "    - The HLSL30 product provides 30 m Nadir normalized Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat-8 OLI data products.  \n",
    "     - Science Dataset (SDS) layers:  \n",
    "        - B05 (NIR)  \n",
    "        - B04 (Red)  \n",
    "        - B02 (Blue)   \n",
    "\n",
    "## Tutorial Outline\n",
    "\n",
    "1. [**Getting Started**](#getstarted)  \n",
    "    1.1 Import Packages  \n",
    "    1.2 EarthData Login\n",
    "    1.3 Set up Dask Client\n",
    "2. [**Finding HLS Data**](#find)  \n",
    "3. [**Opening Data with ODC-STAC**](#odcstac)  \n",
    "    3.1 Apply Scale Factor\n",
    "4. [**Calculating EVI**](#calcevi)   \n",
    "5. [**Quality Filtering**](#qualityfilter)  \n",
    "6. [**Calculating Statistics**](#stats)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started<a id=\"getstarted\"></a>\n",
    "\n",
    "### 1.1 Import Packages\n",
    "\n",
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import dask.distributed\n",
    "import pystac_client\n",
    "import geopandas as gpd\n",
    "import odc.stac\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "import earthaccess\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Earthdata Login Authentication\n",
    "\n",
    "We will use the `earthaccess` package for authentication. `earthaccess` can either create a a new local `.netrc` file to store credentials or validate that one exists already in you user profile. If you do not have a `.netrc` file, you will be prompted for your credentials and one will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into earthaccess - ensures creation of .netrc file\n",
    "earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Set up Dask Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up a local `dask` cluster, which will define tasks based on our lazy loaded data and functions, then split those tasks accross our locally available threads or workers to improve process efficiency. You can view the dashboard by clicking the link and see various dashboards and monitor them as you run future cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask Client\n",
    "client = dask.distributed.Client()\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configure GDAL Options and rio environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure odc.stac rio env - requires a .netrc file, sends info to dask client\n",
    "odc.stac.configure_rio(cloud_defaults=True,\n",
    "                       verbose=True,\n",
    "                       client=client,\n",
    "                       GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "                       GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                       GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CMR-STAC Search<a id=\"find\"></a>\n",
    "\n",
    "To find HLS data, we will use the `pystac_client` python library to search [NASA's Common Metadata Repository SpatioTemporal Asset Catalog (CMR-STAC)](https://cmr.earthdata.nasa.gov/stac/docs/index.html#tag/STAC) for HLS data. We will use a geojson file containing our region of interest (ROI) to search for files that intersect. For this use case, our ROI is an agricultural field in Northern California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open ROI polygon\n",
    "roi = gpd.read_file('../../data/Field_Boundary.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the collection, datetime range, results limit, and simplify our ROI to a bounding box and store these as search parameters. After defining these, conduct a stac search using the `LPCLOUD` STAC endpoint and return our query as a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(\"https://cmr.earthdata.nasa.gov/stac/LPCLOUD\")\n",
    "# Define search parameters\n",
    "search_params = {\n",
    "    \"collections\": [\"HLSS30_2.0\",\"HLSL30_2.0\"],\n",
    "    \"bbox\": tuple(list(roi.total_bounds)),\n",
    "    \"datetime\": \"2021-05-01/2021-09-30\",\n",
    "    \"limit\": 100,\n",
    "}\n",
    "# Perform the search\n",
    "query = catalog.search(**search_params)\n",
    "items = query.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(query.items())\n",
    "print(f\"Found: {len(items):d} granules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can preview what these results look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stack the data from both Landsat and Sentinel instruments, we need common band names for HLSL30 B5 (NIR) and HLSS30 B8A (NIR). We can simply rename them NIR in the stac results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename HLSS B8A and HLSL B05 to common band name\n",
    "for item in items:\n",
    "    if \"HLS.L30\" in item.id:\n",
    "        item.assets[\"NIR\"] = item.assets.pop(\"B05\")\n",
    "    if \"HLS.S30\" in item.id:\n",
    "        item.assets[\"NIR\"] = item.assets.pop(\"B8A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm this changed the stac results\n",
    "items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Opening Data with ODC-STAC<a id=\"odcstac\"></a>\n",
    "\n",
    "Use the `odc.stac.stac_load` function to load the data from the STAC results lazily into a dataset. To do this, we must provide a crs, a list of STAC results, a tuple of the bands we want to load, an expected resolution, and a dask chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CRS and resolution, open lazily with odc.stac\n",
    "crs = \"utm\"\n",
    "ds = odc.stac.stac_load(\n",
    "    items,\n",
    "    bands=(\"B02\", \"B04\",\"NIR\", \"Fmask\"),\n",
    "    crs=crs,\n",
    "    resolution=30,\n",
    "    chunks={\"band\":1,\"x\":512,\"y\":512},  # If empty, chunks along band dim, \n",
    "    #groupby=\"solar_day\", # This limits to first obs per day\n",
    ")\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can preview the size and shapes of the data we have loaded using the `geobox` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Geobox\n",
    "ds.odc.geobox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will clip the data to our ROI using the `rio.clip` function. This will continue to work lazily on the data, so we have yet to actually load any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip\n",
    "ds = ds.rio.clip(roi.geometry.values, roi.crs, all_touched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Clipped Geobox\n",
    "ds.odc.geobox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scale_factor` information in some of the HLSL30 granules are found in the file metadata, but missing from the Band metadata, meaning this isn't applied automatically. Manually scale each of the data arrays by the scale factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "ds.NIR.data = 0.0001 * ds.NIR.data\n",
    "ds.B04.data = 0.0001 * ds.B04.data\n",
    "ds.B02.data = 0.0001 * ds.B02.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clipped and scaled our data, we can load the minimal subset of data we need to calculate our EVI over our time period of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the NIR data to ensure our scaling and clipping worked as expected. We can use the slide bar to scroll through the time dimension showing the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to ensure scaling worked\n",
    "ds.NIR.hvplot.image(x=\"x\", y=\"y\", groupby=\"time\", cmap=\"viridis\", width=600, height=500, crs='EPSG:32610', tiles='ESRI', rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate EVI<a id=\"calcevi\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build an Enhanced Vegetation Index (EVI) dataset using the EVI formula:\n",
    "\n",
    "$$\n",
    "\\text{EVI} = G \\cdot \\frac{\\text{NIR} - \\text{Red}}{\\text{NIR} + C_1 \\cdot \\text{Red} - C_2 \\cdot \\text{Blue} + L}\n",
    "$$\n",
    "\n",
    "**NIR**: Near-Infrared reflectance.  \n",
    "**Red**: Reflectance in the red band.  \n",
    "**Blue**: Reflectance in the blue band, used primarily to correct for aerosol influences.  \n",
    "**G**: Gain factor (often set to 2.5) that scales the index.  \n",
    "**C₁**: Coefficient for the aerosol resistance term using the red band (commonly 6).  \n",
    "**C₂**: Coefficient for the aerosol resistance term using the blue band (commonly 7.5).  \n",
    "**L**: Canopy background adjustment, which helps to minimize soil brightness influences (commonly 1).  \n",
    "\n",
    "We will use the typical values adopted for the MODIS algorithm for G, C1, C2 and L. The output values will range from -1 to 1 and indicate strength of vegetation signal, which correlates with biomass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate EVI\n",
    "evi_ds = 2.5 * ((ds.NIR - ds.B04) / (ds.NIR + 6.0 * ds.B04 - 7.5 * ds.B02 + 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_ds = evi_ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize our EVI over our ROI. We can scroll through the time-series using the slider bar to the right of the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_ds.hvplot.image(x=\"x\", y=\"y\", groupby=\"time\", cmap=\"YlGn\", clim=(0, 1), crs='EPSG:32610', tiles='ESRI', rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Masking<a id=\"qualityfilter\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will apply a function to our EVI dataset, which will mask out pixels based on our bit selection. \n",
    "\n",
    "For HLS v2.0 products, all quality information is included in the Fmask layer. This layer includes values corresponding to combinations of bits that represent\n",
    "different quality descriptions at each location within the scene.  Bits are ordered `76543210` and correspond to the following quality descriptions:  \n",
    "\n",
    "|Bit Number|Mask Name|Bit Value|Description|\n",
    "|---|---|---|---|\n",
    "|7-6|Aerosol<br>Level|11<br>10<br>01<br>00|High<br>Medium<br>Low<br>Clear<br>|\n",
    "|5|Water|1<br>0|Yes<br>No|\n",
    "|4|Snow/Ice|1<br>0|Yes<br>No|\n",
    "|3|Cloud<br>Shadow|1<br>0|Yes<br>No|\n",
    "|2|Cloud/Shadow<br>Adjacent|1<br>0|Yes<br>No|\n",
    "|1|Cloud|1<br>0|Yes<br>No|\n",
    "|0|Cirrus|Reserved|NA|\n",
    "\n",
    "For example, an 8bit integer 224 converted to binary is 11100000. This would indicate high aerosol (bits 7-6), and water (bit 5) are present in a pixel with that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quality_mask(quality_data, bit_nums: list = [1, 2, 3, 4, 5]):\n",
    "    \"\"\"\n",
    "    Uses the Fmask layer and bit numbers to create a binary mask of good pixels.\n",
    "    By default, bits 1-5 are used.\n",
    "    \"\"\"\n",
    "    mask_array = np.zeros((quality_data.shape[0], quality_data.shape[1]))\n",
    "    # Remove/Mask Fill Values and Convert to Integer\n",
    "    quality_data = np.nan_to_num(quality_data.copy(), nan=0).astype(np.int8)\n",
    "    for bit in bit_nums:\n",
    "        # Create a Single Binary Mask Layer\n",
    "        mask_temp = np.array(quality_data) & 1 << bit > 0\n",
    "        mask_array = np.logical_or(mask_array, mask_temp)\n",
    "    return mask_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use xarray's `apply_ufunc` to apply a function in a vectorized way to our EVI timeseries dataset. `bit_nums` can be provided as a dict to specify which bits to mask out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_mask = xr.apply_ufunc(\n",
    "    create_quality_mask,\n",
    "    ds.Fmask,\n",
    "    kwargs={\"bit_nums\": [1,2,3,4,5]},\n",
    "    input_core_dims=[[\"x\", \"y\"]],\n",
    "    output_core_dims=[[\"x\", \"y\"]],\n",
    "    vectorize=True,\n",
    "    dask='parallelized',\n",
    "    output_dtypes=[np.bool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `where` function to mask out regions corresponding with the bit numbers and visualize the EVI timeseries again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_ds.where(~quality_mask).hvplot.image(x=\"x\", y=\"y\", groupby=\"time\", cmap=\"YlGn\", clim=(0, 1), crs='EPSG:32610', tiles='ESRI', rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Info  \n",
    "\n",
    "Email: LPDAAC@usgs.gov  \n",
    "Voice: +1-866-573-3222  \n",
    "Organization: Land Processes Distributed Active Archive Center (LP DAAC)¹  \n",
    "Website: <https://lpdaac.usgs.gov/>  \n",
    "\n",
    "¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls_odc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
