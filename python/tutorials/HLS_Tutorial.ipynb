{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Cloud-Native HLS Data in Python  \n",
    "\n",
    "## Summary  \n",
    "\n",
    "This tutorial was developed to examine changes in enhanced vegetation index (EVI) over an agricultural region in northern California. **The goal of the project is to observe HLS-derived mean EVI over these regions without downloading the entirety of the HLS source data.** In this notebook we will extract an EVI timeseries from Harmonized Landsat Sentinel-2 (HLS) data in the Cloud using the `earthaccess` and `rioxarray` libraries. This tutorial will show how to find the HLS data available in the cloud for our specific time period, bands (layers), and region of interest. After finding the desired data, we will load subsets of the cloud optimized geotiffs (COGs) into a Jupyter Notebook directly from the cloud, and calculate EVI. After calculating EVI we will save and stack the time series, visualize it, and export a CSV of EVI statistics for the region.  \n",
    "\n",
    "## Background  \n",
    "\n",
    "The  Harmonized Landsat Sentinel-2 ([HLS](https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/)) project produces seamless, harmonized surface reflectance data from the Operational Land Imager (OLI) and Multi-Spectral Instrument (MSI) aboard Landsat and Sentinel-2 Earth-observing satellites, respectively. The aim is to produce seamless products with normalized parameters, which include atmospheric correction, cloud and cloud-shadow masking, geographic co-registration and common gridding, normalized bidirectional reflectance distribution function, and spectral band adjustment. This will provide global observation of the Earth’s surface every 2-3 days with 30 meter spatial resolution. One of the major applications that will benefit from HLS is agriculture assessment and monitoring, which is used as the use case for this tutorial.  \n",
    "\n",
    "NASA's Land Processes Distributed Active Archive Center (LP DAAC) archives and distributes HLS products in the LP DAAC Cumulus cloud archive as Cloud Optimized GeoTIFFs (COG). This tutorial will demonstrate  Because these data are stored as COGs, this tutorial will teach users how to load subsets of individual files into memory for just the bands you are interested in--a paradigm shift from the more common workflow where you would need to download a .zip/HDF file containing every band over the entire scene/tile. This tutorial covers how to process HLS data (calculate EVI), visualize, and \"stack\" the scenes over a region of interest into an [xarray](http://xarray.pydata.org/en/stable/) data array, calculate statistics for an EVI time series, and export as a comma-separated values (CSV) file--providing you with all of the information you need for your area of interest without having to download the source data file. The Enhanced Vegetation Index ([EVI](https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_4.php)), is a vegetation index similar to NDVI that has been found to be more sensitive to ground cover below the vegetated canopy and saturates less over areas of dense green vegetation.  \n",
    "\n",
    "## Requirements  \n",
    "\n",
    "- A [NASA Earthdata Login](https://urs.earthdata.nasa.gov/) account is required to download the data used in this tutorial. You can create an account at the link provided.\n",
    "\n",
    "- You will will also need to have a netrc file set up in your home directory in order to successfully run the code below. A code chunk in a later section provides a way to do this, or you can check out the [setup_intstructions.md](../../python/setup/setup_instructions.md).  \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- How to work with HLS Landsat ([HLSL30.002](https://doi.org/10.5067/HLS/HLSL30.002)) and Sentinel-2 ([HLSS30.002](https://doi.org/10.5067/HLS/HLSS30.002)) data products  \n",
    "- How to query and subset HLS data using the `earthaccess` library  \n",
    "- How to access and work with HLS data  \n",
    "\n",
    "## Data Used  \n",
    "\n",
    "- Daily 30 meter (m) global HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance - [HLSS30.002](https://doi.org/10.5067/HLS/HLSS30.002)  \n",
    "    - The HLSS30 product provides 30 m Nadir normalized Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Sentinel-2A and Sentinel-2B MSI data products.  \n",
    "    - Science Dataset (SDS) layers:  \n",
    "        - B8A (NIR Narrow)  \n",
    "        - B04 (Red)  \n",
    "        - B02 (Blue)   \n",
    "- Daily 30 meter (m) global HLS Landsat-8 OLI Surface Reflectance - [HLSL30.002](https://doi.org/10.5067/HLS/HLSL30.002)  \n",
    "    - The HLSL30 product provides 30 m Nadir normalized Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat-8 OLI data products.  \n",
    "     - Science Dataset (SDS) layers:  \n",
    "        - B05 (NIR)  \n",
    "        - B04 (Red)  \n",
    "        - B02 (Blue)   \n",
    "\n",
    "## Tutorial Outline  \n",
    "\n",
    "1. [**Getting Started**](#getstarted)  \n",
    "    1.1 Import Packages  \n",
    "    1.2 EarthData Login  \n",
    "2. [**Finding HLS Data**](#find)  \n",
    "3. [**Accessing HLS COG Data in the Cloud**](#cloudaccess)  \n",
    "    3.1 Subset by Band  \n",
    "    3.2 Load COGS into Memory  \n",
    "    3.3 Subset Spatially  \n",
    "    3.4 Apply Scale Factor  \n",
    "4. [**Processing HLS Data**](#processhls)  \n",
    "    4.1 Subset by Band  \n",
    "    4.2 Calculate EVI  \n",
    "    4.3 Export to COG  \n",
    "5. [**Automation**](#automation)  \n",
    "6. [**Stacking HLS Data**](#stackhls)  \n",
    "    6.1 Open and Stack COGs  \n",
    "    6.2 Visualize Stacked Time Series  \n",
    "7. [**Export Statistics**](#export)  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started<a id=\"getstarted\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import requests as r\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import json\n",
    "import panel as pn\n",
    "import geoviews\n",
    "import earthaccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Earthdata Login Authentication\n",
    "\n",
    "We will use the [`earthaccess`](https://github.com/nsidc/earthaccess#readme) package for authentication. `earthaccess` can either createa a new local `.netrc` file to store credentials or validate that one exists already in you user profile. If you do not have a `.netrc` file, you will be prompted for your credentials and one will be created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding HLS Data using `earthaccess` <a id=\"find\"></a>\n",
    "\n",
    "To find HLS data, we will use the `earthaccess` python library to search NASA's Common Metadata Repository (CMR) for HLS data. We will use an geojson file containing our region of interest (ROI) to search for files that intersect. To do this, we will simplify it to a bounding box. Grab the bounding coordinates from the geopandas object after opening.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in our geojson file using `geopandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = gp.read_file('../../data/Field_Boundary.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `total_bounds` property to get the bounding box of our ROI, and add that to a python tuple, which is the expected data type for the bounding_box parameter `earthaccess` `search_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = tuple(list(field.total_bounds))\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching we can also search a specific time period of interest. Here we search from the beginning of May 2021 to the end of September 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = (\"2021-05-01T00:00:00\", \"2021-09-30T23:59:59\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the HLS collection contains to products, i.e. HLSL30 and HLSS30, we will include both short names. Search using our constraints and the `count = 100` to limit our search to 100 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name=['HLSL30','HLSS30'],\n",
    "    bounding_box=bbox,\n",
    "    temporal=temporal, # 2021-07-15T00:00:00Z/2021-09-15T23:59:59Z\n",
    "    count=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can preview these results in a `pandas` `dataframe` we want to check the metadata. Note we only show the first 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(results).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also preview each individual results by selecting it from the list. This will show the data links, and a browse image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grab all of the URLs for the data using [`list comprehension`](https://www.w3schools.com/python/python_lists_comprehension.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_results_urls = [granule.data_links() for granule in results]\n",
    "hls_results_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the URLs for the browse images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browse_urls = [granule.dataviz_links()[0] for granule in results] # 0 retrieves only the https links\n",
    "browse_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accessing HLS Cloud Optimized GeoTIFFs (COGs) from Earthdata Cloud <a id=\"extracthls\"></a>\n",
    "\n",
    "Now that we have a list of data URLs, we will configure `gdal` and `rioxarray` to access the cloud assets that we are interested in, and read them directly into memory without needing to download the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python libraries used to access COG files in Earthdata Cloud leverage GDAL's virtual file systems. Whether you are running this code in the Cloud or in a local workspace, GDAL configurations must be set in order to successfully access the HLS COG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDAL configurations used to successfully access LP DAAC Cloud Assets via vsicurl \n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF')\n",
    "gdal.SetConfigOption('GDAL_HTTP_UNSAFESSL', 'YES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Subset by Band "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the URLs for one of our returned granules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = hls_results_urls[10]  \n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the EVI for each granule we need the near-infrared, red, and blue bands. Below you can find the different band numbers for each of the two products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel 2:\n",
    "        - \"narrow\" NIR = B8A\n",
    "        - Red = B04\n",
    "        - Blue = B02  \n",
    "### Landsat 8:\n",
    "        - NIR = B05\n",
    "        - Red = B04\n",
    "        - Blue = B02  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will subset our URLs to include the bands identified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_band_links = []\n",
    "\n",
    "# Define which HLS product is being accessed\n",
    "if h[0].split('/')[4] == 'HLSS30.020':\n",
    "    evi_bands = ['B8A', 'B04', 'B02'] # NIR RED BLUE for S30\n",
    "else:\n",
    "    evi_bands = ['B05', 'B04', 'B02'] # NIR RED BLUE for L30\n",
    "\n",
    "# Subset the assets in the item down to only the desired bands\n",
    "for a in h: \n",
    "    if any(b in a for b in evi_bands):\n",
    "        evi_band_links.append(a)\n",
    "evi_band_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from above that you can always quickly load in the browse image to get a quick view of the item using our list of browse URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = io.imread(browse_urls[10])  # Load jpg browse image into memory\n",
    "\n",
    "# Basic plot of the image\n",
    "plt.figure(figsize=(10,10))              \n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see our first observation over the northern Central Valley of California. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del image # Remove the browse image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load HLS COGs into Memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HLS COGs are broken into chunks allowing data to be read more efficiently. Define the chunk size of an HLS tile, mask the NaN values, then read the files using `rioxarray` and name them based upon the band. We also squeeze the object to remove the band dimension from most of the files, since there is only 1 band."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** To scale the bands, you can set the `mask_and_scale` to `True` (`mask_and_scale=True`), however the `scale_factor` information in some of the HLSL30 granules are found in the `file` metadata, but missing from the `Band` metadata. `rioxarray` looks for the `scale_factor` under `Band` metadata and if this information is missing it assumes the `scale_factor` is equal to 1. This results in having data to be uscaled and not masked for those granules. That is why we treat our data a bit differently here, leaving it unscaled and manually updating the `scale_factor` attribute in the `xarray` `dataarray`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vsicurl to load the data directly into memory (be patient, may take a few seconds)\n",
    "chunk_size = dict(band=1, x=512, y=512) # Tiles have 1 band and are divided into 512x512 pixel chunks\n",
    "# Sometimes a vsi curl error occurs so we need to retry if it does\n",
    "max_retries = 10\n",
    "for e in evi_band_links:\n",
    "    print(e)\n",
    "    # Try Loop\n",
    "    for _i in range(max_retries):\n",
    "        try:\n",
    "            # Open and build datasets\n",
    "            if e.rsplit('.', 2)[-2] == evi_bands[0]:      # NIR index\n",
    "                nir = rxr.open_rasterio(e, chunks=chunk_size, masked=True).squeeze('band', drop=True)\n",
    "                nir.attrs['scale_factor'] = 0.0001        # hard coded the scale_factor attribute \n",
    "            elif e.rsplit('.', 2)[-2] == evi_bands[1]:    # red index\n",
    "                red = rxr.open_rasterio(e, chunks=chunk_size, masked=True).squeeze('band', drop=True)\n",
    "                red.attrs['scale_factor'] = 0.0001        # hard coded the scale_factor attribute\n",
    "            elif e.rsplit('.', 2)[-2] == evi_bands[2]:    # blue index\n",
    "                blue = rxr.open_rasterio(e, chunks=chunk_size, masked=True).squeeze('band', drop=True)\n",
    "                blue.attrs['scale_factor'] = 0.0001       # hard coded the scale_factor attribute\n",
    "            break # Break out of the retry loop\n",
    "        except Exception as ex:\n",
    "            print(f\"vsi curl error: {ex}. Retrying...\")\n",
    "    else:\n",
    "        print(f\"Failed to process {e} after {max_retries} retries. Please check to see you're authenticated with earthaccess.\")\n",
    "print(\"The COGs have been loaded into memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Getting an error in the section above? Accessing these files in the cloud requires you to authenticate using your NASA Earthdata Login account. You will need to have a netrc file set up containing those credentials in your home directory in order to successfully run the code below. Please make sure you have a valid username and password in the created netrc file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a quick look at one of the `dataarray` we just read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the full size of the array, **y**=3660 & **x**=3660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Subset spatially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can subset using our input farm field, we will first need to convert the `geopandas` dataframe from lat/lon (EPSG: 4326) into the [projection used by HLS](https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/#hls-tiling-system), UTM (aligned to the Military Grid Reference System). Since UTM is a zonal projection, we'll need to extract the unique UTM zonal projection parameters from our input HLS files and use them to transform the coordinate of our input farm field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the WKT string for our HLS tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir.spatial_ref.crs_wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this information to transform the coordinates of our ROI to the proper UTM projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsUTM = field.to_crs(nir.spatial_ref.crs_wkt) # Take the CRS from the NIR tile that we opened and apply it to our field geodataframe.\n",
    "fsUTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use our field ROI to mask any pixels that fall outside of it and crop to the bounding box using `rasterio`. This greatly reduces the amount of data that are needed to load into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_cropped = nir.rio.clip(fsUTM.geometry.values, fsUTM.crs, all_touched=True) # All touched includes any pixels touched by the polygon\n",
    "nir_cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the array size is considerably smaller than the full size we read in before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the cropped NIR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_cropped.hvplot.image(aspect='equal', cmap='viridis', frame_width= 800, fontscale=1.6, geo=True, tiles='ESRI').opts(title='HLS Cropped NIR Band', )  # Quick visual to assure that it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see that the data have been loaded into memory already subset to our ROI. Also notice that the data has not been scaled (see the legend). We will next scaled the data using the function defined below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Apply Scale Factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to scale \n",
    "def scaling(band):\n",
    "    scale_factor = band.attrs['scale_factor'] \n",
    "    band_out = band.copy()\n",
    "    band_out.data = band.data*scale_factor\n",
    "    band_out.attrs['scale_factor'] = 1\n",
    "    return(band_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_cropped_scaled = scaling(nir_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot to confirm our manual scaling worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_cropped_scaled.hvplot.image(aspect='equal', cmap='viridis', frame_width= 800, fontscale=1.6, geo=True, tiles='ESRI').opts(title='HLS Cropped NIR Band')  # Quick visual to assure that it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load in the red and blue bands and fix their scaling as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red\n",
    "red_cropped = red.rio.clip(fsUTM.geometry.values, fsUTM.crs, all_touched=True)\n",
    "red_cropped_scaled = scaling(red_cropped)\n",
    "# Blue\n",
    "blue_cropped = blue.rio.clip(fsUTM.geometry.values, fsUTM.crs, all_touched=True)\n",
    "blue_cropped_scaled = scaling(blue_cropped)\n",
    "print('Data is loaded and scaled!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Processing HLS Data <a id=\"processhls\"></a>\n",
    "\n",
    "In this section we will use the HLS data we've access to calculate the EVI. We will do this by defining a function to calculate EVI that will retain the attributes and metadata associated with the data we accessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Calculate EVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function we'll use to calculate EVI using the NIR, Red, and Blue bands. The function will: \n",
    "1. build a new `xarray` `dataarray` with EVI values\n",
    "2. copy the original file metadata to the new `xarray` `dataarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_evi(red, blue, nir):\n",
    "      # Create EVI xarray.DataArray that has the same coordinates and metadata\n",
    "      evi = red.copy()\n",
    "      # Calculate the EVI\n",
    "      evi_data = 2.5 * ((nir.data - red.data) / (nir.data + 6.0 * red.data - 7.5 * blue.data + 1.0))\n",
    "      # Replace the Red xarray.DataArray data with the new EVI data\n",
    "      evi.data = evi_data\n",
    "      # exclude the inf values\n",
    "      evi = xr.where(evi != np.inf, evi, np.nan, keep_attrs=True)\n",
    "      # change the long_name in the attributes\n",
    "      evi.attrs['long_name'] = 'EVI'\n",
    "      evi.attrs['scale_factor'] = 1\n",
    "      return evi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, apply the EVI function on the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_cropped = calc_evi(red_cropped_scaled, blue_cropped_scaled, nir_cropped_scaled) # Generate EVI array\n",
    "evi_cropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the results using `hvplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_cropped.hvplot.image(aspect='equal', cmap='YlGn', frame_width= 800, fontscale=1.6, geo=True, tiles='ESRI').opts(title=f'HLS-derived EVI, {evi_cropped.SENSING_TIME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, notice that variation of green level appearing in different fields in our ROI, some being much greener than the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Export to COG \n",
    "\n",
    "In this section, create an output filename and export the cropped EVI to COG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_name = evi_band_links[0].split('/')[-1]\n",
    "original_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard format  for HLS S30 V2.0 and HLS L30 V2.0 filenames is as follows:\n",
    "> **HLS.S30/HLS.L30**: Product Short Name    \n",
    "**T10TEK**: MGRS Tile ID (T+5-digits)  \n",
    "**2020273T190109**: Julian Date and Time of Acquisition (YYYYDDDTHHMMSS)  \n",
    "**v2.0**: Product Version   \n",
    "**B8A/B05**: Spectral Band  \n",
    "**.tif**: Data Format (Cloud Optimized GeoTIFF)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional information on HLS naming conventions, be sure to check out the [HLS Overview Page](https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/#hls-naming-conventions).\n",
    "\n",
    "Now modify the filename to describe that its EVI, cropped to an ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_name = f\"{original_name.split('v2.0')[0]}v2.0_EVI_cropped.tif\"  # Generate output name from the original filename\n",
    "out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `COG` driver to write a local raster output. A cloud-optimized geotiff (COG) is a geotiff file that has been tiled and includes overviews so it can be accessed and previewed without having to load the entire image into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = '../../data/'\n",
    "evi_cropped.rio.to_raster(raster_path=f'{out_folder}{out_name}', driver='COG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del evi_cropped, out_folder, out_name, red_cropped, blue_cropped, nir_cropped, red_cropped_scaled, blue_cropped_scaled, nir_cropped_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Automation <a id=\"automation\"></a>\n",
    "\n",
    "In this section, automate sections 4-5 for each HLS item that intersects our spatiotemporal subset of interest. Loop through each item and subset to the desired bands, load the spatial subset into memory, apply the scale factor, calculate EVI, and export as a Cloud Optimized GeoTIFF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hls_results_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put it all together and loop through each of the files, visualize and export cropped EVI files. \n",
    "Be patient with the for loop below, it may take a few minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, h in enumerate(hls_results_urls):\n",
    "    \n",
    "    outName = h[0].split('/')[-1].split('v2.0')[0] +'v2.0_EVI_cropped.tif'\n",
    "    print(outName)\n",
    "\n",
    "    evi_band_links = []\n",
    "    if h[0].split('/')[4] == 'HLSS30.020':\n",
    "        evi_bands = ['B8A', 'B04', 'B02'] # NIR RED BLUE\n",
    "    else:\n",
    "        evi_bands = ['B05', 'B04', 'B02'] # NIR RED BLUE\n",
    "    \n",
    "    for a in h: \n",
    "        if any(b in a for b in evi_bands):\n",
    "            evi_band_links.append(a)\n",
    "\n",
    "    \n",
    "    # Check if file already exists in output directory, if yes--skip that file and move to the next observation\n",
    "    if os.path.exists(f'../../data/{outName}'):\n",
    "        print(f\"{outName} has already been processed and is available in this directory, moving to next file.\")\n",
    "        continue\n",
    "    \n",
    "    # Use vsicurl to load the data directly into memory (be patient, may take a few seconds)\n",
    "    chunk_size = dict(band=1, x=512, y=512) # Tiles have 1 band and are divided into 512x512 pixel chunks\n",
    "    # Sometimes a vsi curl error occurs so we need to retry if it does\n",
    "    max_retries = 10\n",
    "    for e in evi_band_links:\n",
    "        print(e)\n",
    "        # Try Loop\n",
    "        for _i in range(max_retries):\n",
    "            try:\n",
    "                if e.rsplit('.', 2)[-2] == evi_bands[0]:      # NIR index\n",
    "                    nir = rxr.open_rasterio(e, chunks=chunk_size, masked= True).squeeze('band', drop=True)\n",
    "                    nir.attrs['scale_factor'] = 0.0001                         # hard coded the scale_factor attribute \n",
    "                elif e.rsplit('.', 2)[-2] == evi_bands[1]:    # red index\n",
    "                    red = rxr.open_rasterio(e, chunks=chunk_size, masked= True).squeeze('band', drop=True)\n",
    "                    red.attrs['scale_factor'] = 0.0001                         # hard coded the scale_factor attribute\n",
    "                elif e.rsplit('.', 2)[-2] == evi_bands[2]:    # blue index\n",
    "                    blue = rxr.open_rasterio(e, chunks=chunk_size, masked= True).squeeze('band', drop=True)\n",
    "                    blue.attrs['scale_factor'] = 0.0001                        # hard coded the scale_factor attribute\n",
    "                break # Break out of the retry loop\n",
    "            except Exception as ex:\n",
    "                print(f\"vsi curl error: {ex}. Retrying...\")\n",
    "        else:\n",
    "            print(f\"Failed to process {e} after {max_retries} retries. Please check to see you're authenticated with earthaccess.\")\n",
    "        \n",
    "    fsUTM = field.to_crs(nir.spatial_ref.crs_wkt)\n",
    "\n",
    "    # Crop to our ROI and apply scaling and masking\n",
    "    nir_cropped = nir.rio.clip(fsUTM.geometry.values, fsUTM.crs, all_touched=True)\n",
    "    red_cropped = red.rio.clip(fsUTM.geometry.values, fsUTM.crs, all_touched=True)\n",
    "    blue_cropped = blue.rio.clip(fsUTM.geometry.values, fsUTM.crs, all_touched=True)\n",
    "    \n",
    "    print('Cropped')      \n",
    "    \n",
    "    # Fix Scaling\n",
    "    nir_cropped_scaled = scaling(nir_cropped)\n",
    "    red_cropped_scaled = scaling(red_cropped)\n",
    "    blue_cropped_scaled = scaling(blue_cropped)\n",
    "\n",
    "    # Generate EVI\n",
    "    \n",
    "    evi_cropped = calc_evi(red_cropped_scaled, blue_cropped_scaled, nir_cropped_scaled)\n",
    "\n",
    "    print('EVI Calculated')\n",
    "    \n",
    "    # Remove any observations that are entirely fill value\n",
    "    if np.nansum(evi_cropped.data) == 0.0:\n",
    "        print(f\"File: {h[0].split('/')[-1].rsplit('.', 1)[0]} was entirely fill values and will not be exported.\")\n",
    "        continue\n",
    "        \n",
    "    evi_cropped.rio.to_raster(raster_path=f'../../data/{outName}', driver='COG')\n",
    "    \n",
    "    print(f\"Processed file {j+1} of {len(hls_results_urls)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there should be multiple COGs exported to your working directory, that will be used in Section 6 to stack into a time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stacking HLS Data <a id=\"stackhls\"></a>\n",
    "\n",
    "In this section we will open multiple HLS-derived EVI COGs and stack them into an `xarray` data array along the time dimension. First list the files we created in the `/data/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Open and Stack COGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_dir = '../../data/'\n",
    "evi_files = [os.path.abspath(os.path.join(evi_dir, o)) for o in os.listdir(evi_dir) if o.endswith('EVI_cropped.tif')]  # List EVI COGs\n",
    "evi_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a time index as an xarray variable from the filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_index_from_filenames(evi_files):\n",
    "    '''\n",
    "    Helper function to create a pandas DatetimeIndex\n",
    "    '''\n",
    "    return [datetime.strptime(f.split('.')[-4], '%Y%jT%H%M%S') for f in evi_files]\n",
    "\n",
    "time = xr.Variable('time', time_index_from_filenames(evi_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the cropped HLS COG files are being read using `rioxarray` and a time series stack is created using `xarray`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=dict(band=1, x=512, y=512)\n",
    "\n",
    "evi_ts = xr.concat([rxr.open_rasterio(f, mask_and_scale=True, chunks=chunks).squeeze('band', drop=True) for f in evi_files], dim=time)\n",
    "evi_ts.name = 'EVI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_ts = evi_ts.sortby(evi_ts.time)\n",
    "evi_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualize Stacked Time Series\n",
    "\n",
    "Below, use the [`hvPlot`](https://hvplot.pyviz.org/index.html) and [`holoviews`](https://www.holoviews.org/) packages to create an interactive time series plot of the HLS derived EVI data.Basemap layer is also added to provide better context of the areas surrounding our region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell generates a warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# set the x, y, and z (groupby) dimensions, add a colormap/bar and other parameters.\n",
    "title = 'HLS-derived EVI over agricultural fields in northern California'\n",
    "evi_ts.hvplot.image(x='x', y='y', groupby = 'time', frame_height=500, frame_width= 500, cmap='YlGn', geo=True, tiles = 'EsriImagery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Looking at the time series above, the farm fields are pretty stable in terms of EVI during our temporal range. The slow decrease in EVI as we move toward Fall season could show these fields are having some sort of trees rather than crops. I encourage you to expand your temporal range to learn more about the EVI annual and seasonal changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is in an xarray we can intuitively slice or reduce the dataset. Let's select a single time slice from the EVI variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use slicing to plot data only for a specific observation, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'HLS-derived EVI over agricultural fields in northern California'\n",
    "# evi_cropped.hvplot.image(aspect='equal', cmap='YlGn', frame_width=300).opts(title=f'HLS-derived EVI, {evi_cropped.SENSING_TIME}', clabel='EVI')\n",
    "\n",
    "evi_ts.isel(time=1).hvplot.image(x='x', y='y', cmap='YlGn', geo=True, tiles = 'EsriImagery', frame_height=500, frame_width= 500).opts(title=f'{title}, {evi_ts.isel(time=4).SENSING_TIME}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the time series as boxplots showing the distribution of EVI values for our farm field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_ts.hvplot.box('EVI', by=['time'], rot=90, box_fill_color='lightblue', width=800, height=600).opts(ylim=(-0.5,1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics shows a relatively stable green status in these fields during mid May to the end of September 2021. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Statistics<a id=\"export\"></a>\n",
    "\n",
    "Next, calculate statistics for each observation and export to CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarray allows you to easily calculate a number of statistics\n",
    "evi_min = evi_ts.min(('y', 'x'))\n",
    "evi_max = evi_ts.max(('y', 'x'))\n",
    "evi_mean = evi_ts.mean(('y', 'x'))\n",
    "evi_sd = evi_ts.std(('y', 'x'))\n",
    "evi_count = evi_ts.count(('y', 'x'))\n",
    "evi_median = evi_ts.median(('y', 'x'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the `mean` and `standard deviation` for each time slice as well as the `maximum` and `minimum` values. Let's do some plotting! We will use the [`hvPlot`](https://hvplot.pyviz.org/index.html) package to create simple but interactive charts/plots. Hover your curser over the visualization to see the data values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_mean.hvplot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Combine line plots for different statistics\n",
    "stats = (evi_mean.hvplot.line(height=350, width=450, line_width=1.5, color='red', grid=True, padding=0.05).opts(title='Mean')+ \n",
    "    evi_sd.hvplot.line(height=350, width=450, line_width=1.5, color='red', grid=True, padding=0.05).opts(title='Standard Deviation')\n",
    "    + evi_max.hvplot.line(height=350, width=450, line_width=1.5, color='red', grid=True, padding=0.05).opts(title='Max') + \n",
    "    evi_min.hvplot.line(height=350, width=450, line_width=1.5, color='red', grid=True, padding=0.05).opts(title='Min')).cols(2)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these graphs are also interactive--hover over the line to see the value for a given date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a `pandas` dataframe with the statistics, and export to a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe from dictionary\n",
    "df = pd.DataFrame({'Min EVI': evi_min, 'Max EVI': evi_max, \n",
    "                   'Mean EVI': evi_mean, 'Standard Deviation EVI': evi_sd, \n",
    "                   'Median EVI': evi_median, 'Count': evi_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = evi_ts.time.data                       # Set the observation date as the index\n",
    "df.to_csv('../../data/HLS-Derived_EVI_Stats.csv', index=True)  # Export to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! You have now not only learned how to get started with HLS V2.0 data, but have also learned how to navigate cloud-native data `earthaccess`, how to access subsets of COGs, and how to write COGs for your own outputs. Using this jupyter notebook as a workflow, you should now be able to switch to your specific region of interest and re-run the notebook. Good Luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Info  \n",
    "\n",
    "Email: LPDAAC@usgs.gov  \n",
    "Voice: +1-866-573-3222  \n",
    "Organization: Land Processes Distributed Active Archive Center (LP DAAC)¹  \n",
    "Website: <https://lpdaac.usgs.gov/>  \n",
    "Date last modified: 01-17-2024  \n",
    "\n",
    "¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
